time_slot=5, num_his=1, num_pred=1, L=1, K=8, d=8, train_size=200, val_ratio=0.1, test_ratio=0.2, batch_size=50, max_epoch=100, patience=10, learning_rate=0.01, decay_epoch=10, traffic_file='../data/so2/data.h5', SE_file='data/so2/SE.txt', model_file='model/so2.pt', log_file='logs/so2_train', output_file='data/so2/preds.npz'
loading data...
fullX: torch.Size([364, 1, 30])		 fullY: torch.Size([364, 2, 1])
trainX: torch.Size([200, 1, 30])		 trainY: torch.Size([200, 1, 30])
valX:   torch.Size([92, 1, 30])		valY:   torch.Size([92, 1, 30])
testX:   torch.Size([73, 1, 30])		testY:   torch.Size([73, 1, 30])
trainTE:   torch.Size([200, 2, 1])		valTE:   torch.Size([92, 2, 1])
testTE:   torch.Size([73, 2, 1])		fullTE:   torch.Size([364, 1, 30])
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-07-30 13:14:08 | epoch: 0001/100, training time: 1.6s, inference time: 0.2s
train loss: 291.3526, val_loss: 212.9270
val loss decrease from inf to 212.9270, saving model to model/so2.pt
2022-07-30 13:14:10 | epoch: 0002/100, training time: 1.2s, inference time: 0.2s
train loss: 284.9974, val_loss: 192.6438
val loss decrease from 212.9270 to 192.6438, saving model to model/so2.pt
2022-07-30 13:14:11 | epoch: 0003/100, training time: 1.1s, inference time: 0.2s
train loss: 282.4750, val_loss: 222.0848
2022-07-30 13:14:12 | epoch: 0004/100, training time: 1.2s, inference time: 0.2s
train loss: 279.9072, val_loss: 262.2486
2022-07-30 13:14:14 | epoch: 0005/100, training time: 1.2s, inference time: 0.2s
train loss: 276.4842, val_loss: 286.6923
2022-07-30 13:14:15 | epoch: 0006/100, training time: 1.2s, inference time: 0.2s
train loss: 273.3022, val_loss: 265.6277
2022-07-30 13:14:16 | epoch: 0007/100, training time: 1.1s, inference time: 0.2s
train loss: 270.3396, val_loss: 279.5839
2022-07-30 13:14:18 | epoch: 0008/100, training time: 1.5s, inference time: 0.2s
train loss: 268.5261, val_loss: 282.8874
2022-07-30 13:14:20 | epoch: 0009/100, training time: 1.1s, inference time: 0.3s
train loss: 266.1882, val_loss: 281.7818
2022-07-30 13:14:21 | epoch: 0010/100, training time: 1.1s, inference time: 0.2s
train loss: 263.9175, val_loss: 287.2834
2022-07-30 13:14:22 | epoch: 0011/100, training time: 1.2s, inference time: 0.2s
train loss: 262.2955, val_loss: 280.9374
2022-07-30 13:14:24 | epoch: 0012/100, training time: 1.2s, inference time: 0.2s
train loss: 260.3438, val_loss: 285.3061
2022-07-30 13:14:25 | epoch: 0013/100, training time: 1.1s, inference time: 0.2s
train loss: 258.8278, val_loss: 272.9630
2022-07-30 13:14:26 | epoch: 0014/100, training time: 1.1s, inference time: 0.2s
train loss: 256.7920, val_loss: 279.9779
2022-07-30 13:14:27 | epoch: 0015/100, training time: 1.1s, inference time: 0.2s
train loss: 255.1585, val_loss: 266.4467
2022-07-30 13:14:29 | epoch: 0016/100, training time: 1.4s, inference time: 0.3s
train loss: 253.5358, val_loss: 273.5627
2022-07-30 13:14:31 | epoch: 0017/100, training time: 1.6s, inference time: 0.3s
train loss: 250.9311, val_loss: 266.8761
2022-07-30 13:14:33 | epoch: 0018/100, training time: 1.5s, inference time: 0.2s
train loss: 249.9482, val_loss: 264.1469
2022-07-30 13:14:34 | epoch: 0019/100, training time: 1.1s, inference time: 0.2s
train loss: 250.3570, val_loss: 266.3392
2022-07-30 13:14:35 | epoch: 0020/100, training time: 1.1s, inference time: 0.2s
train loss: 247.3496, val_loss: 253.9758
2022-07-30 13:14:37 | epoch: 0021/100, training time: 1.1s, inference time: 0.2s
train loss: 245.9559, val_loss: 273.0781
2022-07-30 13:14:38 | epoch: 0022/100, training time: 1.3s, inference time: 0.2s
train loss: 243.9748, val_loss: 257.6371
2022-07-30 13:14:40 | epoch: 0023/100, training time: 1.2s, inference time: 0.2s
train loss: 242.3833, val_loss: 267.0960
2022-07-30 13:14:41 | epoch: 0024/100, training time: 1.1s, inference time: 0.2s
train loss: 240.8401, val_loss: 272.2628
2022-07-30 13:14:42 | epoch: 0025/100, training time: 1.2s, inference time: 0.2s
train loss: 239.2112, val_loss: 259.5106
2022-07-30 13:14:44 | epoch: 0026/100, training time: 1.9s, inference time: 0.2s
train loss: 238.2971, val_loss: 266.5924
2022-07-30 13:14:46 | epoch: 0027/100, training time: 1.1s, inference time: 0.2s
train loss: 236.2023, val_loss: 257.4407
2022-07-30 13:14:47 | epoch: 0028/100, training time: 1.1s, inference time: 0.2s
train loss: 234.8117, val_loss: 266.5768
2022-07-30 13:14:48 | epoch: 0029/100, training time: 1.2s, inference time: 0.2s
train loss: 233.9205, val_loss: 262.9901
2022-07-30 13:14:50 | epoch: 0030/100, training time: 1.4s, inference time: 0.2s
train loss: 233.0767, val_loss: 266.2428
2022-07-30 13:14:51 | epoch: 0031/100, training time: 1.2s, inference time: 0.2s
train loss: 231.1621, val_loss: 262.9730
2022-07-30 13:14:52 | epoch: 0032/100, training time: 1.1s, inference time: 0.2s
train loss: 231.6364, val_loss: 254.7195
2022-07-30 13:14:54 | epoch: 0033/100, training time: 1.1s, inference time: 0.2s
train loss: 231.3704, val_loss: 260.9523
2022-07-30 13:14:55 | epoch: 0034/100, training time: 1.2s, inference time: 0.2s
train loss: 229.6189, val_loss: 260.5388
2022-07-30 13:14:56 | epoch: 0035/100, training time: 1.1s, inference time: 0.2s
train loss: 227.8379, val_loss: 268.2754
2022-07-30 13:14:58 | epoch: 0036/100, training time: 1.1s, inference time: 0.2s
train loss: 226.6617, val_loss: 259.9317
2022-07-30 13:14:59 | epoch: 0037/100, training time: 1.2s, inference time: 0.2s
train loss: 225.0221, val_loss: 268.8129
2022-07-30 13:15:00 | epoch: 0038/100, training time: 1.1s, inference time: 0.2s
train loss: 223.5821, val_loss: 253.0683
2022-07-30 13:15:02 | epoch: 0039/100, training time: 1.1s, inference time: 0.2s
train loss: 222.8587, val_loss: 263.5551
2022-07-30 13:15:03 | epoch: 0040/100, training time: 1.1s, inference time: 0.2s
train loss: 221.3468, val_loss: 255.9620
2022-07-30 13:15:04 | epoch: 0041/100, training time: 1.1s, inference time: 0.2s
train loss: 221.2749, val_loss: 262.6160
2022-07-30 13:15:05 | epoch: 0042/100, training time: 1.1s, inference time: 0.2s
train loss: 219.5049, val_loss: 254.0970
2022-07-30 13:15:07 | epoch: 0043/100, training time: 1.1s, inference time: 0.2s
train loss: 218.9459, val_loss: 256.6728
2022-07-30 13:15:08 | epoch: 0044/100, training time: 1.1s, inference time: 0.2s
train loss: 217.8913, val_loss: 257.3417
2022-07-30 13:15:09 | epoch: 0045/100, training time: 1.1s, inference time: 0.2s
train loss: 216.5054, val_loss: 250.6420
2022-07-30 13:15:11 | epoch: 0046/100, training time: 1.1s, inference time: 0.2s
train loss: 215.3045, val_loss: 261.5760
2022-07-30 13:15:12 | epoch: 0047/100, training time: 1.1s, inference time: 0.2s
train loss: 214.0839, val_loss: 256.0495
2022-07-30 13:15:13 | epoch: 0048/100, training time: 1.1s, inference time: 0.2s
train loss: 212.8290, val_loss: 256.7041
2022-07-30 13:15:14 | epoch: 0049/100, training time: 1.1s, inference time: 0.2s
train loss: 211.6002, val_loss: 262.3037
2022-07-30 13:15:16 | epoch: 0050/100, training time: 1.1s, inference time: 0.2s
train loss: 211.6734, val_loss: 249.9198
2022-07-30 13:15:17 | epoch: 0051/100, training time: 1.1s, inference time: 0.2s
train loss: 210.2146, val_loss: 251.1018
2022-07-30 13:15:18 | epoch: 0052/100, training time: 1.2s, inference time: 0.2s
train loss: 209.1064, val_loss: 253.0159
2022-07-30 13:15:20 | epoch: 0053/100, training time: 1.1s, inference time: 0.2s
train loss: 208.6670, val_loss: 257.2203
2022-07-30 13:15:21 | epoch: 0054/100, training time: 1.1s, inference time: 0.2s
train loss: 207.4403, val_loss: 248.7150
2022-07-30 13:15:22 | epoch: 0055/100, training time: 1.1s, inference time: 0.2s
train loss: 207.4985, val_loss: 247.9620
2022-07-30 13:15:24 | epoch: 0056/100, training time: 1.1s, inference time: 0.2s
train loss: 206.9242, val_loss: 244.1903
2022-07-30 13:15:25 | epoch: 0057/100, training time: 1.1s, inference time: 0.2s
train loss: 206.2365, val_loss: 249.6781
2022-07-30 13:15:26 | epoch: 0058/100, training time: 1.1s, inference time: 0.2s
train loss: 204.6081, val_loss: 238.0708
2022-07-30 13:15:28 | epoch: 0059/100, training time: 1.7s, inference time: 0.3s
train loss: 204.0845, val_loss: 254.9710
2022-07-30 13:15:30 | epoch: 0060/100, training time: 1.6s, inference time: 0.2s
train loss: 202.9733, val_loss: 246.3004
2022-07-30 13:15:32 | epoch: 0061/100, training time: 2.0s, inference time: 0.3s
train loss: 202.8064, val_loss: 244.8092
2022-07-30 13:15:35 | epoch: 0062/100, training time: 2.7s, inference time: 0.4s
train loss: 201.4769, val_loss: 256.8980
2022-07-30 13:15:40 | epoch: 0063/100, training time: 4.0s, inference time: 0.5s
train loss: 201.4445, val_loss: 245.0911
2022-07-30 13:15:42 | epoch: 0064/100, training time: 1.5s, inference time: 0.2s
train loss: 200.2343, val_loss: 249.0850
2022-07-30 13:15:44 | epoch: 0065/100, training time: 1.9s, inference time: 0.6s
train loss: 200.2170, val_loss: 242.7931
2022-07-30 13:15:48 | epoch: 0066/100, training time: 2.9s, inference time: 0.7s
train loss: 198.2662, val_loss: 244.4229
2022-07-30 13:15:52 | epoch: 0067/100, training time: 3.4s, inference time: 0.5s
train loss: 198.4913, val_loss: 244.1806
2022-07-30 13:15:56 | epoch: 0068/100, training time: 3.6s, inference time: 0.4s
train loss: 197.0460, val_loss: 243.1710
2022-07-30 13:15:59 | epoch: 0069/100, training time: 3.0s, inference time: 0.5s
train loss: 196.7257, val_loss: 236.8784
2022-07-30 13:16:03 | epoch: 0070/100, training time: 3.3s, inference time: 0.5s
train loss: 195.3957, val_loss: 237.2193
2022-07-30 13:16:07 | epoch: 0071/100, training time: 3.6s, inference time: 0.6s
train loss: 195.0170, val_loss: 236.3273
2022-07-30 13:16:09 | epoch: 0072/100, training time: 1.7s, inference time: 0.3s
train loss: 194.2103, val_loss: 237.8469
2022-07-30 13:16:11 | epoch: 0073/100, training time: 1.3s, inference time: 0.2s
train loss: 193.8834, val_loss: 236.2615
2022-07-30 13:16:13 | epoch: 0074/100, training time: 1.5s, inference time: 0.5s
train loss: 192.7499, val_loss: 237.8937
2022-07-30 13:16:14 | epoch: 0075/100, training time: 1.5s, inference time: 0.2s
train loss: 192.2120, val_loss: 232.0559
2022-07-30 13:16:16 | epoch: 0076/100, training time: 1.5s, inference time: 0.2s
train loss: 191.3743, val_loss: 235.6265
2022-07-30 13:16:18 | epoch: 0077/100, training time: 1.6s, inference time: 0.3s
train loss: 191.1248, val_loss: 234.7950
2022-07-30 13:16:20 | epoch: 0078/100, training time: 1.2s, inference time: 0.4s
train loss: 190.6786, val_loss: 235.3534
2022-07-30 13:16:21 | epoch: 0079/100, training time: 1.6s, inference time: 0.3s
train loss: 189.8869, val_loss: 236.8480
2022-07-30 13:16:23 | epoch: 0080/100, training time: 1.3s, inference time: 0.2s
train loss: 188.9103, val_loss: 230.5284
2022-07-30 13:16:25 | epoch: 0081/100, training time: 1.8s, inference time: 0.2s
train loss: 189.3520, val_loss: 239.8102
2022-07-30 13:16:27 | epoch: 0082/100, training time: 1.8s, inference time: 0.2s
train loss: 189.0313, val_loss: 232.7064
2022-07-30 13:16:29 | epoch: 0083/100, training time: 1.5s, inference time: 0.2s
train loss: 187.1913, val_loss: 236.8527
2022-07-30 13:16:31 | epoch: 0084/100, training time: 1.9s, inference time: 0.3s
train loss: 186.8063, val_loss: 234.9709
2022-07-30 13:16:33 | epoch: 0085/100, training time: 1.4s, inference time: 0.2s
train loss: 185.9884, val_loss: 233.8195
2022-07-30 13:16:34 | epoch: 0086/100, training time: 1.1s, inference time: 0.2s
train loss: 185.6329, val_loss: 232.6199
2022-07-30 13:16:35 | epoch: 0087/100, training time: 1.1s, inference time: 0.3s
train loss: 185.7272, val_loss: 236.2623
2022-07-30 13:16:37 | epoch: 0088/100, training time: 1.1s, inference time: 0.2s
train loss: 185.9770, val_loss: 232.2311
2022-07-30 13:16:38 | epoch: 0089/100, training time: 1.1s, inference time: 0.2s
train loss: 184.2405, val_loss: 230.5007
2022-07-30 13:16:40 | epoch: 0090/100, training time: 1.8s, inference time: 0.2s
train loss: 183.4238, val_loss: 233.0010
2022-07-30 13:16:41 | epoch: 0091/100, training time: 1.1s, inference time: 0.2s
train loss: 182.9560, val_loss: 232.8163
2022-07-30 13:16:43 | epoch: 0092/100, training time: 1.3s, inference time: 0.2s
train loss: 182.7173, val_loss: 234.8628
2022-07-30 13:16:44 | epoch: 0093/100, training time: 1.2s, inference time: 0.2s
train loss: 182.0886, val_loss: 232.3776
2022-07-30 13:16:46 | epoch: 0094/100, training time: 1.3s, inference time: 0.2s
train loss: 182.2332, val_loss: 229.4022
2022-07-30 13:16:47 | epoch: 0095/100, training time: 1.5s, inference time: 0.3s
train loss: 180.9227, val_loss: 235.1942
2022-07-30 13:16:49 | epoch: 0096/100, training time: 1.6s, inference time: 0.4s
train loss: 180.6021, val_loss: 230.7123
2022-07-30 13:16:52 | epoch: 0097/100, training time: 1.8s, inference time: 0.4s
train loss: 180.2521, val_loss: 229.9907
2022-07-30 13:16:53 | epoch: 0098/100, training time: 1.5s, inference time: 0.2s
train loss: 180.1257, val_loss: 233.3728
2022-07-30 13:16:55 | epoch: 0099/100, training time: 1.5s, inference time: 0.2s
train loss: 179.4819, val_loss: 227.5732
2022-07-30 13:16:57 | epoch: 0100/100, training time: 1.2s, inference time: 0.3s
train loss: 178.5726, val_loss: 236.0249
Training is completed, and model has been stored as model/so2.pt
**** testing model ****
loading model from model/so2.pt
model restored!
evaluating...
test             4.07		14.26		nan%
performance in each prediction step
step: 01         4.07		14.26		nan%
average:         4.07		14.26		nan%
